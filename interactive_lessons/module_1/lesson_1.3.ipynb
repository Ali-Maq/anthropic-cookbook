{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéì Lesson 1.3: Controlling Outputs\n",
    "\n",
    "## üìö What You'll Learn\n",
    "\n",
    "By the end of this lesson, you'll understand:\n",
    "- How temperature affects randomness\n",
    "- What max_tokens does and how to use it\n",
    "- Stop sequences for controlling output\n",
    "- Getting consistent, predictable results\n",
    "- Top-p and top-k sampling\n",
    "\n",
    "**Time to Complete**: 40-50 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## üé≤ Understanding Randomness\n",
    "\n",
    "Claude doesn't just give the \"one correct answer\" - it's probabilistic! Imagine Claude as a creative writer:\n",
    "\n",
    "**Question**: \"The cat sat on the...\"\n",
    "\n",
    "**Possible completions**:\n",
    "- mat (70% likely)\n",
    "- chair (15% likely)\n",
    "- roof (10% likely)\n",
    "- skateboard (5% likely)\n",
    "\n",
    "**Temperature** controls how adventurous Claude is in choosing words!\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from anthropic import Anthropic\n",
    "\n",
    "load_dotenv()\n",
    "client = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "print(\"‚úÖ Client ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üå°Ô∏è Temperature: Controlling Creativity\n",
    "\n",
    "Temperature ranges from 0 to 1:\n",
    "- **0.0**: Deterministic, predictable (always picks the most likely word)\n",
    "- **0.5**: Balanced (default for many uses)\n",
    "- **1.0**: Creative, random (takes more risks)\n",
    "\n",
    "Let's see the difference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same question, different temperatures\n",
    "question = \"Write a creative opening line for a sci-fi story.\"\n",
    "\n",
    "# Temperature = 0 (Very predictable)\n",
    "response_cold = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    max_tokens=100,\n",
    "    temperature=0.0,  # Deterministic!\n",
    "    messages=[{\"role\": \"user\", \"content\": question}]\n",
    ")\n",
    "\n",
    "# LINE-BY-LINE EXPLANATION:\n",
    "# ---------------------------\n",
    "# temperature=0.0\n",
    "#   This makes Claude VERY conservative\n",
    "#   It will always pick the most probable next word\n",
    "#   Results are consistent - run it 10 times, get similar answers\n",
    "#   Good for: factual questions, data extraction, code generation\n",
    "\n",
    "print(\"ü•∂ TEMPERATURE = 0.0 (Conservative)\")\n",
    "print(response_cold.content[0].text)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature = 1 (Very creative)\n",
    "response_hot = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    max_tokens=100,\n",
    "    temperature=1.0,  # Maximum creativity!\n",
    "    messages=[{\"role\": \"user\", \"content\": question}]\n",
    ")\n",
    "\n",
    "# LINE-BY-LINE EXPLANATION:\n",
    "# ---------------------------\n",
    "# temperature=1.0\n",
    "#   This makes Claude VERY creative and unpredictable\n",
    "#   It considers less likely word choices\n",
    "#   Results vary significantly between runs\n",
    "#   Good for: creative writing, brainstorming, diverse outputs\n",
    "\n",
    "print(\"üî• TEMPERATURE = 1.0 (Creative)\")\n",
    "print(response_hot.content[0].text)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run the SAME question multiple times with different temperatures\n",
    "def test_temperature(temp, runs=3):\n",
    "    \"\"\"Test how temperature affects output consistency.\"\"\"\n",
    "    print(f\"\\nüå°Ô∏è Testing Temperature = {temp}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i in range(runs):\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=50,\n",
    "            temperature=temp,\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Name a color.\"}]\n",
    "        )\n",
    "        print(f\"Run {i+1}: {response.content[0].text}\")\n",
    "\n",
    "# Test low temperature (should be very similar)\n",
    "test_temperature(0.0)\n",
    "\n",
    "# Test high temperature (should be varied)\n",
    "test_temperature(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° When to Use Each Temperature\n",
    "\n",
    "| Temperature | Use Case | Example |\n",
    "|------------|----------|----------|\n",
    "| 0.0 - 0.3 | Facts, data extraction, code | \"Extract the email from this text\" |\n",
    "| 0.4 - 0.7 | Balanced responses | \"Explain quantum physics\" |\n",
    "| 0.8 - 1.0 | Creative writing, brainstorming | \"Write a poem\" |\n",
    "\n",
    "---\n",
    "\n",
    "## üé´ Max Tokens: Setting Length Limits\n",
    "\n",
    "Remember: **1 token ‚âà 4 characters** or **¬æ of a word**\n",
    "\n",
    "Max tokens controls the MAXIMUM length of Claude's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very short response\n",
    "response_short = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    max_tokens=20,  # Very short!\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Explain artificial intelligence in detail.\"}]\n",
    ")\n",
    "\n",
    "print(\"üìè MAX_TOKENS = 20 (Very Short)\")\n",
    "print(response_short.content[0].text)\n",
    "print(f\"\\nActual tokens used: {response_short.usage.output_tokens}\")\n",
    "print(f\"Stop reason: {response_short.stop_reason}\")  # Will be 'max_tokens'!\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Longer response\n",
    "response_long = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    max_tokens=500,  # Much more room\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Explain artificial intelligence in detail.\"}]\n",
    ")\n",
    "\n",
    "print(\"üìè MAX_TOKENS = 500 (Longer)\")\n",
    "print(response_long.content[0].text)\n",
    "print(f\"\\nActual tokens used: {response_long.usage.output_tokens}\")\n",
    "print(f\"Stop reason: {response_long.stop_reason}\")  # Might be 'end_turn'\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Understanding Stop Reasons\n",
    "\n",
    "- **end_turn**: Claude finished naturally (complete thought)\n",
    "- **max_tokens**: Hit the token limit (response was cut off)\n",
    "- **stop_sequence**: Hit a custom stop sequence (explained below)\n",
    "\n",
    "If you see `max_tokens` as the stop reason, increase `max_tokens`!\n",
    "\n",
    "---\n",
    "\n",
    "## üõë Stop Sequences: Custom Stop Points\n",
    "\n",
    "Stop sequences tell Claude \"when you see THIS text, STOP generating.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop at a specific word\n",
    "response_stop = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    max_tokens=500,\n",
    "    stop_sequences=[\"However\"],  # Stop if Claude says \"However\"\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"List the pros and cons of electric cars.\"\n",
    "    }]\n",
    ")\n",
    "\n",
    "# LINE-BY-LINE EXPLANATION:\n",
    "# ---------------------------\n",
    "# stop_sequences=[\"However\"]\n",
    "#   This is a LIST of strings\n",
    "#   When Claude generates any of these strings, it STOPS immediately\n",
    "#   Useful for structured output (e.g., stop at \"###\" or \"END\")\n",
    "#   The stop sequence is NOT included in the output\n",
    "\n",
    "print(\"üõë WITH STOP SEQUENCE: ['However']\")\n",
    "print(response_stop.content[0].text)\n",
    "print(f\"\\nStop reason: {response_stop.stop_reason}\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical example: Generate a numbered list that stops at item 5\n",
    "response_list = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    max_tokens=500,\n",
    "    stop_sequences=[\"6.\"],  # Stop before item 6\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"List programming languages. Number each one.\"\n",
    "    }]\n",
    ")\n",
    "\n",
    "print(\"üõë Stop at item 6:\")\n",
    "print(response_list.content[0].text)\n",
    "print(f\"\\nStop reason: {response_list.stop_reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Stop Sequence Use Cases\n",
    "\n",
    "1. **Structured output**: Stop at delimiters (\"###\", \"---\", \"END\")\n",
    "2. **Limiting lists**: Stop at a specific item number\n",
    "3. **Sections**: Stop when a new section starts\n",
    "4. **Dialogue**: Stop after one character speaks\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Top-p (Nucleus Sampling)\n",
    "\n",
    "Top-p is another way to control randomness (alternative to temperature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-p example\n",
    "response_topp = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    max_tokens=200,\n",
    "    top_p=0.1,  # Only consider top 10% most likely tokens\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Describe a sunset in poetic language.\"\n",
    "    }]\n",
    ")\n",
    "\n",
    "# LINE-BY-LINE EXPLANATION:\n",
    "# ---------------------------\n",
    "# top_p=0.1\n",
    "#   'p' stands for probability\n",
    "#   0.1 means \"only consider tokens in the top 10% of probability\"\n",
    "#   Lower top_p = more focused, less random\n",
    "#   Higher top_p = more diverse\n",
    "#   Range: 0.0 to 1.0\n",
    "#   Don't use both temperature AND top_p at the same time!\n",
    "\n",
    "print(\"üéØ TOP-P = 0.1 (Focused)\")\n",
    "print(response_topp.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§î Temperature vs Top-p\n",
    "\n",
    "**Use temperature when**:\n",
    "- You want simple control (0 = boring, 1 = creative)\n",
    "- You're familiar with the concept\n",
    "\n",
    "**Use top-p when**:\n",
    "- You want more precise control over randomness\n",
    "- You're doing advanced sampling\n",
    "\n",
    "**Don't use both at the same time!** Pick one or the other.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Practice Exercise 1: Temperature Experiments\n",
    "\n",
    "**Task**: Test how temperature affects creative vs factual outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Test different temperatures\n",
    "\n",
    "def compare_temperatures(question, temps=[0.0, 0.5, 1.0]):\n",
    "    \"\"\"Compare the same question at different temperatures.\"\"\"\n",
    "    for temp in temps:\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=100,\n",
    "            temperature=temp,\n",
    "            messages=[{\"role\": \"user\", \"content\": question}]\n",
    "        )\n",
    "        print(f\"\\nüå°Ô∏è Temperature {temp}:\")\n",
    "        print(response.content[0].text)\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "# Test 1: Creative question (should show big differences)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 1: Creative Question\")\n",
    "print(\"=\"*60)\n",
    "compare_temperatures(\"Invent a new ice cream flavor.\")\n",
    "\n",
    "# Test 2: Factual question (should be similar)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 2: Factual Question\")\n",
    "print(\"=\"*60)\n",
    "compare_temperatures(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Practice Exercise 2: Token Management\n",
    "\n",
    "**Task**: Create a function that ensures complete responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Build a smart token manager\n",
    "\n",
    "def smart_ask(question, initial_tokens=100, max_retries=3):\n",
    "    \"\"\"\n",
    "    Ask Claude a question and ensure we get a complete answer.\n",
    "    If we hit max_tokens, retry with more tokens.\n",
    "    \"\"\"\n",
    "    tokens = initial_tokens\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=tokens,\n",
    "            messages=[{\"role\": \"user\", \"content\": question}]\n",
    "        )\n",
    "        \n",
    "        # Check if we got a complete answer\n",
    "        if response.stop_reason == \"end_turn\":\n",
    "            print(f\"‚úÖ Complete answer in {tokens} tokens!\")\n",
    "            return response.content[0].text\n",
    "        \n",
    "        # If cut off, try again with more tokens\n",
    "        print(f\"‚ö†Ô∏è Response cut off at {tokens} tokens. Retrying with more...\")\n",
    "        tokens *= 2  # Double the tokens\n",
    "    \n",
    "    print(f\"‚ùå Still incomplete after {max_retries} attempts.\")\n",
    "    return response.content[0].text\n",
    "\n",
    "# Test it!\n",
    "result = smart_ask(\"Explain the history of the internet in detail.\", initial_tokens=50)\n",
    "print(\"\\nFinal result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Practice Exercise 3: Stop Sequences for Structured Data\n",
    "\n",
    "**Task**: Use stop sequences to extract only what you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a recipe but stop after ingredients\n",
    "recipe_response = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    max_tokens=500,\n",
    "    stop_sequences=[\"Instructions:\", \"Directions:\"],\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Write a recipe for chocolate chip cookies. Include ingredients and instructions.\"\n",
    "    }]\n",
    ")\n",
    "\n",
    "print(\"üç™ Recipe (Ingredients Only):\")\n",
    "print(recipe_response.content[0].text)\n",
    "print(f\"\\nStop reason: {recipe_response.stop_reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Generate a story but stop after the first paragraph\n",
    "\n",
    "story_response = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    max_tokens=500,\n",
    "    stop_sequences=[\"\\n\\n\"],  # Stop at double newline (paragraph break)\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Write a mystery story about a detective.\"\n",
    "    }]\n",
    ")\n",
    "\n",
    "print(\"üìñ Story (First Paragraph Only):\")\n",
    "print(story_response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Challenge: Build a Consistent Data Extractor\n",
    "\n",
    "**Goal**: Extract structured data consistently using low temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a contact info extractor\n",
    "\n",
    "def extract_contact_info(text):\n",
    "    \"\"\"\n",
    "    Extract name, email, and phone from text.\n",
    "    Uses low temperature for consistency.\n",
    "    \"\"\"\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20241022\",\n",
    "        max_tokens=200,\n",
    "        temperature=0.0,  # Deterministic!\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Extract the contact information from this text.\n",
    "Return ONLY in this format:\n",
    "Name: [name]\n",
    "Email: [email]\n",
    "Phone: [phone]\n",
    "\n",
    "Text: {text}\"\"\"\n",
    "        }]\n",
    "    )\n",
    "    return response.content[0].text\n",
    "\n",
    "# Test it multiple times - should be identical!\n",
    "sample_text = \"Hi, I'm John Doe. You can reach me at john.doe@email.com or call 555-1234.\"\n",
    "\n",
    "print(\"Testing consistency with temperature=0.0...\\n\")\n",
    "for i in range(3):\n",
    "    print(f\"Run {i+1}:\")\n",
    "    print(extract_contact_info(sample_text))\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Best Practices Summary\n",
    "\n",
    "### Temperature\n",
    "- ‚úÖ Use 0.0 for: Data extraction, coding, factual Q&A\n",
    "- ‚úÖ Use 0.7-1.0 for: Creative writing, brainstorming\n",
    "- ‚ùå Don't use temperature with top_p\n",
    "\n",
    "### Max Tokens\n",
    "- ‚úÖ Set higher than needed, let Claude finish naturally\n",
    "- ‚úÖ Check `stop_reason` to verify completeness\n",
    "- ‚úÖ Use `usage.output_tokens` to track actual length\n",
    "- ‚ùå Don't set too low and cut off responses\n",
    "\n",
    "### Stop Sequences\n",
    "- ‚úÖ Use for structured output\n",
    "- ‚úÖ Use to limit length precisely\n",
    "- ‚úÖ Can have multiple stop sequences\n",
    "- ‚ùå Don't make them too common (might stop too early)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Lesson Complete!\n",
    "\n",
    "### What You Learned:\n",
    "- ‚úÖ How temperature controls randomness (0 = consistent, 1 = creative)\n",
    "- ‚úÖ Max tokens sets length limits\n",
    "- ‚úÖ Stop sequences control output endpoints\n",
    "- ‚úÖ Top-p for advanced sampling\n",
    "- ‚úÖ When to use each parameter\n",
    "- ‚úÖ Building consistent extractors\n",
    "\n",
    "### Key Concepts:\n",
    "1. **Temperature = creativity dial** (0 to 1)\n",
    "2. **Max tokens = length limit** (check stop_reason)\n",
    "3. **Stop sequences = custom endpoints** (for structured output)\n",
    "4. **Low temp = consistency** (for data extraction)\n",
    "\n",
    "### Next Steps:\n",
    "üìñ **Lesson 1.4**: Working with JSON - Learn to get perfectly structured data!\n",
    "\n",
    "---\n",
    "\n",
    "## ü§î Reflection Questions\n",
    "\n",
    "1. When would you use temperature=0.0 vs temperature=1.0?\n",
    "2. What happens if max_tokens is too small?\n",
    "3. How can you tell if a response was cut off?\n",
    "4. Give 3 examples of good stop sequences.\n",
    "5. Why shouldn't you use temperature and top_p together?\n",
    "\n",
    "Ready to continue? Open `lesson_1.4.ipynb`!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
